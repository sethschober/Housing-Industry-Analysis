{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This analysis seeks to gather and prove valuable observations about the impact of various home attributes on its value. The project combines multiple publicly available datasets [made available](https://data.kingcounty.gov/) by King County. One dataset provides a record of home and land sales alongside various identifying feautres. The second dataset gives very granular data about housing in King County, going beyond just square footage to a breakdown by room, many identifying features such as porch size, and more. Combining these two datasets opens the door for the analysis to come. \n",
    "\n",
    "This notebook provides a somewhat condensed analysis compared to the full sequence necessary to understand the full details of choosing specific models and the nitty-gritty details. Please refer to the notebooks in the repository folder notebooks->exploratory if you would like to see a deep dive.\n",
    "\n",
    "### The Process\n",
    "1. Basic Setup and Data Assembly\n",
    "2. Data Aggregation and Cleaning\n",
    "3. Feature Selection and Creation\n",
    "4. The Model\n",
    "5. Quantify The Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup and Data Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "# Import functions from a Python file in this repository with context-relevant functionality\n",
    "path_to_src = os.path.join('..', '..', 'src')\n",
    "sys.path.insert(1, path_to_src)\n",
    "from custom_functions import *\n",
    "pd.set_option('display.max_columns', 100)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import King County housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv files from the data directory\n",
    "df_lookup = pd.read_csv(os.path.join('..','..', 'data', 'raw', 'EXTR_LookUp.csv'), dtype='str')\n",
    "df_resbldg = pd.read_csv(os.path.join('..','..', 'data', 'raw', 'EXTR_ResBldg.csv'), dtype='str')\n",
    "df_rpsale = pd.read_csv(os.path.join('..','..', 'data', 'raw', 'EXTR_RpSale.csv'), dtype='str')\n",
    "\n",
    "# Use the user-defined strip_spaces function to remove leading and trailing spaces from the entire dataframe\n",
    "df_lookup = strip_spaces(df_lookup)\n",
    "df_resbldg = strip_spaces(df_resbldg)\n",
    "df_rpsale = strip_spaces(df_rpsale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Aggregation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sales Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eliminate unecessary data. After close investigation, the below columns were deemed the most worthy of continued analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual selection of the features of choice\n",
    "rpsale_desired_columns = ['ExciseTaxNbr', 'Major', 'Minor', 'DocumentDate', 'SalePrice', 'RecordingNbr', 'PropertyType', \n",
    "                          'PrincipalUse', 'SaleInstrument', 'AFForestLand', 'AFCurrentUseLand', 'AFNonProfitUse', \n",
    "                          'AFHistoricProperty', 'SaleReason', 'PropertyClass', 'SaleWarning']\n",
    "\n",
    "# Remove all columns that are not in the above lists\n",
    "df_rpsale = df_rpsale[rpsale_desired_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create identifier that will be used to connect the two dataframes. \n",
    "In this case, each database provides *Major* and *Minor*, which serve as location-specific identifiers. From here on, the combination of *Major* and *Minor* will simply be referred to as the *parcel*. Although there is often more than one sale associated with a parcel, this is a great place to start for narrowing down our search. The goal is to narrow down the *Sales* dataset to include only one sale per parcel. This allows for a connection with the second database, *Residential Buildings*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParcelID\n",
    "df_rpsale['Parcel_ID'] = df_rpsale.Major + '-' + df_rpsale.Minor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Sales Database\n",
    "Starting with some of the nitty gritty data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only sales for \"Residential\" plots, corresponding to code #6, as can be found in the data dictionary\n",
    "# This eliminates Commerical, Condominium, Apartment, etc.\n",
    "df_rpsale['PrincipalUse'] = elimination_by_code(df_rpsale['PrincipalUse'], '6')\n",
    "\n",
    "# PropertyClass is another distinction between Commerical/Industrial and Residential, as well as \n",
    "# other fundamental features. Code #8 corresponds to Residential Improved property\n",
    "df_rpsale['PropertyClass'] = elimination_by_code(df_rpsale['PropertyClass'], '8')\n",
    "\n",
    "# Yet another classification of property type. Code #11 corresponds to single family households\n",
    "# Here we eliminate multiple family residences, alongside many commercial uses\n",
    "df_rpsale['PropertyType'] = elimination_by_code(df_rpsale['PropertyType'], '11')\n",
    "df_rpsale.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limit scope to 2019 sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type conversion\n",
    "df_rpsale['DocumentDate'] = df_rpsale.DocumentDate.astype(np.datetime64)\n",
    "\n",
    "# Isolate SaleYear as its own column\n",
    "df_rpsale['SaleYear'] = [sale.year for sale in df_rpsale['DocumentDate']]\n",
    "\n",
    "# Eliminate rows corresponding to sales in a year other than 2019\n",
    "df_rpsale = df_rpsale.loc[df_rpsale['SaleYear']==2019].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eliminate unrealistically small sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_acceptable_sale_price = 25000\n",
    "df_rpsale['SalePrice'] = df_rpsale.SalePrice.astype('int')\n",
    "df_rpsale = df_rpsale.loc[df_rpsale.SalePrice > min_acceptable_sale_price].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create column to identify duplicates, a necessary process before combining the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpsale['SaleCount'] = list(map(dict(df_rpsale.Parcel_ID.value_counts()).get, df_rpsale.Parcel_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upon further inspection, there are still duplicates (cases of more than one sale of a given parcel in 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    17884\n",
      "2      682\n",
      "6        6\n",
      "3        6\n",
      "Name: SaleCount, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_rpsale.SaleCount.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eliminate sales that are not the most recent\n",
    "This eliminates the duplicated data issue of multiple sales in one year, bringing the dataset one step closer to a unique Parcel ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined function that returns True/False indicating whether a sale is the most recent for each parcel\n",
    "true_false = identify_latest_sale(df_rpsale.DocumentDate, df_rpsale.Parcel_ID)\n",
    "df_rpsale = df_rpsale.loc[true_false].copy()\n",
    "\n",
    "# Recalculate 'SaleCount' after removing old sales as described above\n",
    "df_rpsale['SaleCount'] = list(map(dict(df_rpsale.Parcel_ID.value_counts()).get, df_rpsale.Parcel_ID))\n",
    "print(df_rpsale.SaleCount.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The great majority of parcels are narrowed to one sale. For the remainders, take a simple approach: average the SalePrice for all sales on that parcel. \n",
    "Further inspection was done to verify that this is a valid way of dealing with outliers. For example, in many cases, the sales are of equal or nearly equal price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined function to return the average SalePrice for each parcel with more than one sale\n",
    "df_rpsale = avg_price_for_duped_parcels(df_rpsale)\n",
    "df_rpsale['SaleCount'] = list(map(dict(df_rpsale.Parcel_ID.value_counts()).get, df_rpsale.Parcel_ID))\n",
    "\n",
    "\n",
    "# Remove duplicates\n",
    "df_rpsale.index = df_rpsale.Parcel_ID.values\n",
    "df_rpsale = df_rpsale.drop_duplicates('Parcel_ID')\n",
    "\n",
    "# Verify no duplicates remain\n",
    "print(df_rpsale.SaleCount.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As a final step to cleaning *Sales* dataset, eliminate columns that are of no use going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns as final step of cleaning before join\n",
    "cols_to_keep = ['SalePrice', 'Parcel_ID', 'PropertyType', 'PrincipalUse', \n",
    "                'SaleInstrument', 'AFForestLand', 'AFCurrentUseLand', \n",
    "                'AFNonProfitUse', 'AFHistoricProperty', 'SaleReason', 'PropertyClass', 'SaleWarning']\n",
    "df_rpsale = df_rpsale[cols_to_keep].copy()\n",
    "df_rpsale.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Buildings Database\n",
    "This dataset requires far less cleaning than *Sales* did. In this case, it is mostly eliminating columns that have been determined not valuable as well as converting to correct datatypes.\n",
    "\n",
    "##### Eliminate unecessary data. After close investigation, the below columns were deemed the most worthy of continued analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual selection of the features of choice\n",
    "resbldg_desired_columns = ['Major', 'Minor', 'NbrLivingUnits', 'Stories', 'BldgGrade', \n",
    "                           'BldgGradeVar', 'SqFt1stFloor', 'SqFtHalfFloor', 'SqFt2ndFloor',\n",
    "                           'SqFtUpperFloor', 'SqFtUnfinFull', 'SqFtUnfinHalf', 'SqFtTotLiving', 'SqFtTotBasement', \n",
    "                           'SqFtFinBasement', 'FinBasementGrade', 'SqFtGarageBasement', 'SqFtGarageAttached', \n",
    "                           'DaylightBasement','SqFtOpenPorch', 'SqFtEnclosedPorch', 'SqFtDeck', 'HeatSystem',\n",
    "                           'HeatSource', 'BrickStone', 'ViewUtilization', 'Bedrooms','BathHalfCount', \n",
    "                           'Bath3qtrCount', 'BathFullCount', 'FpSingleStory','FpMultiStory', 'FpFreestanding', \n",
    "                           'FpAdditional', 'YrBuilt','YrRenovated', 'PcntComplete', 'Obsolescence', \n",
    "                           'PcntNetCondition','Condition']\n",
    "\n",
    "\n",
    "# Remove all columns that are not in one of the above two lists.\n",
    "df_resbldg = df_resbldg[resbldg_desired_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Parcel ID, which will map to Sales database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParcelID is an aggregation of Major and Minor, as it was with the Sales database\n",
    "df_resbldg['Parcel_ID'] = df_resbldg.Major + '-' + df_resbldg.Minor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert features to the appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each of these columns into integer format\n",
    "convert_to_int = ['SqFtOpenPorch', 'SqFtEnclosedPorch', 'Bedrooms', 'SqFtGarageAttached', 'SqFtGarageBasement', 'NbrLivingUnits', 'BldgGrade', 'SqFt1stFloor',\n",
    "                  'SqFtHalfFloor', 'SqFt2ndFloor', 'SqFtUpperFloor', 'SqFtUnfinFull', 'SqFtUnfinHalf','SqFtTotLiving', 'SqFtTotBasement', 'SqFtFinBasement', \n",
    "                  'FinBasementGrade', 'SqFtGarageBasement', 'SqFtGarageAttached', 'SqFtOpenPorch', 'SqFtEnclosedPorch', 'SqFtDeck', 'BathHalfCount', \n",
    "                  'Bath3qtrCount', 'BathFullCount', 'FpSingleStory', 'FpMultiStory', 'FpFreestanding', 'FpAdditional', 'YrBuilt', 'YrRenovated', 'BrickStone']\n",
    "\n",
    "# Loop over entire list\n",
    "for category in convert_to_int:\n",
    "    df_resbldg[category] = df_resbldg[category].astype('int')\n",
    "    \n",
    "    \n",
    "# Convert into float format\n",
    "df_resbldg['Stories'] = df_resbldg['Stories'].astype('float')\n",
    "\n",
    "# Nit-picky\n",
    "df_resbldg['DaylightBasement'] = df_resbldg['DaylightBasement'].str.upper() # Data cleaning for inconsistent casing\n",
    "df_resbldg = df_resbldg.loc[df_resbldg.PcntComplete.astype('str') == '0'].copy() # Remove buildings that aren't complete\n",
    "df_resbldg = df_resbldg.loc[df_resbldg.Obsolescence.astype('str') == '0'].copy() # Remove buildings in obsolescence process\n",
    "df_resbldg = df_resbldg.loc[df_resbldg.PcntNetCondition.astype('str') == '0'].copy() # Remove 6 outliers in abnormal condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine *Sales* and *Buildings* datasets\n",
    "At long last: conduct join using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection with a SQL DataBase file\n",
    "path_to_db = os.path.join('..', '..', 'data', 'main.db')\n",
    "connection = sqlite3.connect(path_to_db)\n",
    "df_resbldg.to_sql('buildings', connection, if_exists='replace')\n",
    "df_rpsale.to_sql('sales', connection, if_exists='replace')\n",
    "\n",
    "# Join Sales and Buildings data together\n",
    "query = '''SELECT * FROM buildings LEFT JOIN sales USING (Parcel_ID)'''\n",
    "df_main = pd.read_sql(query, connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove columns that are not of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_delete = ['Major', 'Minor', 'BldgGradeVar', 'PcntComplete', 'Obsolescence', 'PcntNetCondition', 'Parcel_ID', \n",
    "                  'index', 'PropertyType', 'PrincipalUse', 'SaleInstrument', 'AFForestLand', 'AFCurrentUseLand', 'AFNonProfitUse',\n",
    "                  'AFHistoricProperty', 'SaleReason', 'SaleWarning', 'index']\n",
    "\n",
    "df_main.drop(cols_to_delete, axis=1, inplace=True)\n",
    "df_main.dropna(inplace=True)\n",
    "df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limit study to homes with one living unit\n",
    "\n",
    "As can be seen below, nearly all of the remaining homes have only one living unit. For the sake of consistency, limit the analysis to only single-living unit homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of homes in analysis, categorized by the number of living units:')\n",
    "print(df_main.NbrLivingUnits.value_counts())\n",
    "\n",
    "df_main = df_main.loc[df_main.NbrLivingUnits == 1].copy()\n",
    "df_main.drop('NbrLivingUnits', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Creation\n",
    "Using the above data, categorical inputs will be one-hot encoded such that they can be useful components of the linear regression model. Note that additional variables will be removed along this process. For the sake of keeping this notebook brief, please refer to the exploratory notebooks for further investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand Square Footage metrics and remove unnecessary or redundant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two metrics that uncompass the same idea: unfinished area of the home\n",
    "df_main['SqFtUnfinished'] = df_main['SqFtUnfinFull'] + df_main['SqFtUnfinHalf']\n",
    "\n",
    "# # Additionally, create a metric for Unfinished basement space, which will be used to analyze finished vs unfinished basements \n",
    "df_main['SqFtUnfinBasement'] = df_main.SqFtTotBasement - df_main.SqFtFinBasement\n",
    "\n",
    "# Remove SquareFootage columns determined to be redundant\n",
    "sq_ft_cols_to_drop = ['SqFt1stFloor', 'SqFtHalfFloor', 'SqFtUpperFloor', 'SqFtUnfinFull', 'SqFtUnfinHalf', 'SqFt2ndFloor']\n",
    "df_main.drop(sq_ft_cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate *Bathroom* data\n",
    "The original dataset puts bathrooms into three categories: Full, Half, and 3/4. The goal is to combine them into one column, Bathrooms. Method 1 is to count each bathroom the same, regardless of size. Method 2 is to assign them fractional counts based on their naming (1/2, 3/4). The below correlation plot shows that the results is nearly the same, a negligible difference. For the purposes of this analysis, we will use Method 2 so as to not ignore the difference in value that may be added by having a smaller bathroom (e.g. \"half bath\"). As we can see in the correlation plot below, the two methods are very highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bathrooms for Method 1 and Method 2, as described above\n",
    "bathrooms_v1 = df_main[['BathHalfCount', 'Bath3qtrCount', 'BathFullCount']].sum(axis=1)\n",
    "bathrooms_v2 = df_main['BathHalfCount']/2 + df_main['Bath3qtrCount']*3/4 + df_main['BathFullCount']\n",
    "\n",
    "# Combine the two methods into one dataframe and visualize their correlation\n",
    "bathrooms = pd.concat([bathrooms_v1, bathrooms_v2], axis=1)\n",
    "bathrooms.columns = ['Method 1', 'Method 2']\n",
    "corr = bathrooms.corr()\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.title('Comparing Two Methods of Calculating the Total Bathrooms')\n",
    "\n",
    "# Remove columns that are now uncompassed by singluar \"Bathrooms\" feature\n",
    "bathroom_cols_to_drop = ['BathHalfCount', 'Bath3qtrCount', 'BathFullCount']\n",
    "df_main.drop(bathroom_cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Add \"Bathrooms\" feature, Method 2, to the main dataframe to continue the analysis\n",
    "df_main['Bathrooms'] = bathrooms_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate *Fireplace* data\n",
    "Similar to Bathrooms, the original data source breaks down the number of Fireplaces into multiple categories: single story, multi-story, freestanding, and additional. The below code combines them into one final feature, corresponding to the total number of fireplaces. The correlation matrix is added to see how the *Total* correlates with *SalePrice*, as opposed to each individual. \n",
    "The results show the *Total Fireplaces* has a higher correlation with *SalePrice* than any of it's components. This shows that we're not ignoring potential important relationships, while also aggregating data that was disaggregated to an extent that it wouldn't be especially helpful as a result of the final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all fireplace data in one spot and calculate the total\n",
    "fireplaces = df_main[['FpSingleStory', 'FpMultiStory', 'FpFreestanding', 'FpAdditional']].copy()\n",
    "fireplaces['FpTotal'] = fireplaces.sum(axis=1)\n",
    "\n",
    "# Reorder to make Correlation Matrix easier to understand\n",
    "fireplaces = fireplaces[['FpTotal', 'FpSingleStory', 'FpMultiStory', 'FpFreestanding', 'FpAdditional']]\n",
    "\n",
    "# Test correlation of Total with individuals\n",
    "corr = pd.concat([df_main.SalePrice, fireplaces], axis=1).corr()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "sns.heatmap(corr, annot=True, ax=axes[0]);\n",
    "axes[0].set_title('Correlation of Fireplace Metrics with SalePrice')\n",
    "\n",
    "sns.distplot(fireplaces.FpTotal, ax=axes[1])\n",
    "axes[1].set_title('Total Fireplaces per Home')\n",
    "axes[1].set_ylabel('Number of Homes (Thousands)')\n",
    "\n",
    "\n",
    "# Drop columns determined to be less valuable\n",
    "fp_cols_to_drop = ['FpSingleStory', 'FpMultiStory', 'FpFreestanding', 'FpAdditional']\n",
    "df_main.drop(fp_cols_to_drop, axis=1, inplace=True)\n",
    "df_main['Fireplaces'] = fireplaces['FpTotal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "For categorical features of interest, convert to one-hot encoded columns\n",
    "#### Encode HeatSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert encoded values to text values to understand their meaning\n",
    "keys = get_lookups(108)\n",
    "convert_to_keys = lambda x: keys[str(int(x))].replace(\" \", \"\").replace('-','') if x!='0' else 'Other'\n",
    "heating = df_main.HeatSystem.apply(convert_to_keys)\n",
    "print('Original breakdown:\\n{}'.format(heating.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gravity heating is very uncommon. Lump it into the \"Other\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify = lambda x: 'Other' if x in ['Gravity'] else x\n",
    "heating = heating.apply(simplify)\n",
    "print('Improved categorization:\\n{}'.format(heating.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encode remaining categories\n",
    "Drop the ForcedAir option. Since it is by far the most common, it will be considered the default option upon which to base alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "heating_dummies = pd.get_dummies(heating, prefix='Heating', drop_first=False)\n",
    "\n",
    "# Drop ForcedAir instead of whatever happens to be the first column\n",
    "heating_dummies.drop('Heating_ForcedAir', axis=1, inplace=True) \n",
    "\n",
    "#Adjust df_main accordingly\n",
    "df_main = pd.concat([df_main, heating_dummies], axis=1)\n",
    "df_main.drop(['HeatSystem', 'Heating_Other'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Condition\n",
    "Follow process similar to HeatSystem to represent categorical *Condition* feature. Drop the *Average* option since it is naturally a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numerical encoding to actual values\n",
    "keys = get_lookups(83)\n",
    "convert_to_keys = lambda x: keys[x].replace(' ', '')\n",
    "condition = df_main.Condition.apply(convert_to_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a breakdown to show commonality of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_cts = pd.DataFrame(condition.value_counts()).T.reset_index(drop=True)\n",
    "condition_cts = condition_cts[['Poor', 'Fair', 'Average', 'Good', 'VeryGood']].T #Reorder\n",
    "plt.bar(condition_cts.index, condition_cts.iloc[:, 0])\n",
    "plt.title('Commonality of \"Condition\" Classifications')\n",
    "plt.ylabel('Number of Homes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encode Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "condition_dummies = pd.get_dummies(condition, prefix='Condition', drop_first=False)\n",
    "\n",
    "# Drop Average instead of whatever happens to be the first column\n",
    "condition_dummies.drop('Condition_Average', axis=1, inplace=True)\n",
    "\n",
    "#Adjust df_main accordingly\n",
    "df_main.drop('Condition', axis=1, inplace=True)\n",
    "df_main = pd.concat([df_main, condition_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Porch\n",
    "By default, the information given about porches is given in square feet. Instead of looking at the impact of porch size, we will investigate whether its presence impacts SalePrice. Encoded as either enclosed, open, both, or neither. The default will be \"neither\", and as a result we will drop that column once encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrame to temporarily hold Porch data\n",
    "porches = df_main[['SqFtOpenPorch', 'SqFtEnclosedPorch']].copy()\n",
    "porches.columns = ['Open', 'Encl']\n",
    "\n",
    "classify_porches = lambda x: 'Both' if ((x.Open>0)&(x.Encl>0)) else 'Open' if x.Open>0 else 'Closed' if x.Encl>0 else 'None'\n",
    "porches['Porch'] = porches.apply(classify_porches, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the commonality of Porches\n",
    "As we can see, Open porches are by far the most common type, but it is just as common for a home to not have a porch at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porch_cts = pd.DataFrame(porches.Porch.value_counts())\n",
    "\n",
    "print('Commonality of Porches\\n', porch_cts)\n",
    "plt.bar(porch_cts.index, porch_cts.Porch)\n",
    "plt.title('Commonality of \"Porch\" Types')\n",
    "plt.ylabel('Number of Homes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encode Porch types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porches_dummies = pd.get_dummies(porches['Porch'], prefix='Porch', drop_first=False)\n",
    "porches_dummies.drop('Porch_None', axis=1, inplace=True)\n",
    "\n",
    "# Adjust df_main accordingly\n",
    "df_main = pd.concat([df_main, porches_dummies], axis=1)\n",
    "df_main.drop(['SqFtOpenPorch', 'SqFtEnclosedPorch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create binary column for whether a home has been renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x: 1 if x>0 else 0\n",
    "df_main['Renovated'] = df_main.YrRenovated.apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df_main.YrRenovated)\n",
    "axes[0].set_title(\"Year Renovated (Zero Indicates Never Renovated)\")\n",
    "axes[0].set_ylabel('Number of Homes');\n",
    "\n",
    "axes[1].hist(df_main.loc[df_main.YrRenovated>0, 'YrRenovated'])\n",
    "axes[1].set_title('Year Renovated *For Homes That Have Been Renovated*')\n",
    "axes[1].set_ylabel('Number of Homes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add binary indication of renovation (1 = \"yes\", 0 = \"no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x: 1 if x>0 else 0\n",
    "df_main['Renovated'] = df_main.YrRenovated.apply(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create binary column for whether a home's basement is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basement = df_main[['SqFtFinBasement', 'SqFtUnfinBasement']].copy()\n",
    "basement.columns = ['fin', 'unfin']\n",
    "classify_basements = lambda x: 'Unfinished' if ((x.fin>0)&(x.unfin>0)) else 'Finished' if x.fin>0 else 'Unfinished' if x.unfin>0 else 'None'\n",
    "basement['Simplified'] = basement.apply(classify_basements, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot commonality of whether a basement is finished, unfinished, or simply does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basement_cts = pd.DataFrame(basement.Simplified.value_counts())\n",
    "plt.bar(basement_cts.index, basement_cts.Simplified)\n",
    "plt.title('Commonality of Basement Statuses')\n",
    "plt.ylabel('Number of Homes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode\n",
    "basement_dummies = pd.get_dummies(basement.Simplified, prefix='Basement', drop_first=False)\n",
    "\n",
    "# Drop 'Unfinished' to set it as the default when investigating the impact of finishing the basement\n",
    "basement_dummies.drop('Basement_Unfinished', axis=1)\n",
    "\n",
    "df_main = pd.concat([df_main, basement_dummies], axis=1)\n",
    "df_main.drop('Basement_None', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Misc. cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df_main.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Move dependent variable to first column to make future graphs more comprehensible\n",
    "df_main = pd.concat([df_main['SalePrice'], df_main.drop('SalePrice', axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Model\n",
    "Below is an investigation of modeling the above-formulated data via inferential linear regression. Only the first and final models are included for the sake of brevity. For a full investigation of the impact of adding specific features, please review the previously referenced Exploratory notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data\n",
    "##### In table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of columns remaining for analysis:', list(df_main.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graphical: Distribution relationships between features\n",
    "The below pair-plot shows only continous features. Features with a small number of values don't provide much meaning in this graph. It is also a slow function, which is made slower by including all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Seaborn warnings that have previously been determined non-consequential\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# User-defined function that isolates inputs with a minimum number of distinct observations\n",
    "continuous_features = identify_continuous_features(df_main)\n",
    "\n",
    "# Distribution Plots for above-derived features\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 10))\n",
    "for col, ax in zip(continuous_features.columns, axes.flatten()):\n",
    "    sns.distplot(continuous_features[col], ax=ax)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('')\n",
    "    plt.subplots_adjust(wspace=.5, hspace=.5)\n",
    "fig.suptitle('Distribution of Continuous Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical: Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heat map for continuous features (as defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = continuous_features.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool)) # Formula taken from Flatiron School study group material\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.heatmap(corr, mask=mask, ax=ax, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heat map for features with few distinct inputs (i.e. those note above), plotted with SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discrete = pd.concat([df_main.SalePrice, df_main.drop(continuous_features.columns, axis=1)], axis=1)\n",
    "\n",
    "corr = df_discrete.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool)) # Formula taken from Flatiron School study group material\n",
    "fig, ax = plt.subplots(figsize=(18, 14))\n",
    "sns.heatmap(corr, mask=mask, ax=ax, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List correlations of SalePrice and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df_main.corr().stack().reset_index()\n",
    "correlations = correlations.loc[(correlations.level_0 == 'SalePrice') & (correlations.level_1 != 'SalePrice')]\n",
    "correlations.columns = ['Dependent', 'Feature', 'Correlation (abs)']\n",
    "correlations['Correlation (abs)'] = correlations['Correlation (abs)'].abs()\n",
    "correlations.sort_values(by='Correlation (abs)', ascending=False, inplace=True, ignore_index=True)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Baseline Model\n",
    "After reviewing the above table, we see that Livable Square Footage is the highest predictor or Sale Price. This isn't surprising. Let's create the simple model and check the assumptions of Linear Regression. Let's also create a dataframe to store the model performance metrics so we can compare them as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics_cols = ['Y', 'X', 'Linearity p-value', 'Jarque-Bera (JB) metric', 'JB p-value', 'Lagrange multiplier', 'Lagrange multiplier p-value', 'F-score', 'F-score p-value', 'Average VIF', 'R^2 (Adj.)']\n",
    "performance_metrics = pd.DataFrame(columns = performance_metrics_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['SqFtTotLiving']\n",
    "output = 'SalePrice'\n",
    "\n",
    "model, df_model = produce_model(df_main, inputs, output)\n",
    "results = check_assumptions(model, df_model, output, verbose=True, feature_to_plot='SqFtTotLiving')\n",
    "performance_metrics = performance_metrics.append(results)\n",
    "\n",
    "print(model.summary())\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of above model\n",
    "As we can see, it's not linear. In fact, only one of the necessary fundamental assumptions is met. That said, it is impossible to not meet given one input feature. That is of course Multicollinearity. We see that in the Variance Inflation Factor (VIF) producing an NA. All other statistics are especially bad:\n",
    "1. **Linearity: failed.** The inputs are not linear, as described by the linear rainbow test. A p-value greater than 0.05 would indicate linearity. This model produces a p-value of 0.000004. Not good.\n",
    "2. **Normality of Residuals: failed.** The model residuals are not normally distributed. This is seen in the QQ-plot and the Jarque-Bera (JB) metric and its associated p-value. Although the metric would ideally be less than 5 and the p-value would be greater than 0.05, we instead have a metric equal to 13168580 and a p-value equal to effectively 0. Not good.\n",
    "3. **Homoscedacity: failed.** The residuals are not homoscedastic. This is visualized in the scatter plot above and quantified by the F-score and its associated p-value. Lower F-scores are better, and the p-value should be above 0.05. Instead, the F-score in 439 and the associated p-value is 2.18e-96. Not good.\n",
    "4. **Multi-collinearity: passed by default.** this is inherently not an issue as discussed above solely because there is only one input, thus making collinearity impossible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "We see that there are extensive improvements to be made to the above model. For a full walk-through of the discovery process of getting to this final model, please review the Exploratory notebooks. In brief, here are some improvements made to the model:\n",
    "1. SalePrice is log-transformed. We can see in the distribution plot above that there is a severe skew to the SalePrice data. This is nearly entirely reconciled by log-transforming the output, as is done in the final model.\n",
    "2. SqFtTotLiving is also log-transformed for a very similar reason. It improves linearity drastically. \n",
    "3. Binary feature corresponding to whether or not the basement has been finished was added.\n",
    "4. One-hot encoded features corresponding to the porch are added, measuring the impact of an Open porch, and Enclosed porch, or Both.\n",
    "5. One-hot encoded features corresponding to the type of heating used in the home are added. \n",
    "\n",
    "As a result, all performance metrics have improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log-transform SqFtLivingArea and SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform = lambda x: np.log(x)    \n",
    "df_main['SalePrice_log'] = df_main.SalePrice.apply(log_transform)\n",
    "df_main['SqFtTotLiving_log'] = df_main.SqFtTotLiving.apply(log_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.009/0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (move this down)\n",
    "\n",
    "##### Analysis of the final model\n",
    "There have in fact been substantial improvements in all metrics necessary to meet the fundamental assumptions of linear regression. Two of four have reached the necessary threshold. The other two have not, but have got closer.\n",
    "1. **Linearity: passed.** The linearity assumption is met because the p-value associated with the linear rainbow test is 0.37, far above the predetermined 0.05 alpha threshold required. This improvement came largely from log-transforming the SalePrice and the SqFtLivingArea, however, there were plenty of features that were tested in the model development process that were excluded in part because they pummeled the linearity p-value.\n",
    "2. **Normality of Residuals: failed.** The normality of residuals, measured by the JB metric and p-value, have a ways to go. The JB stat decreased from 1.3e7 to 6300. This is a huge improvement, but the associated p-value still rounds to zero, meaning it is far from checking the box.\n",
    "3. **Homoscedacity: failed but *almost* passed.** The plot above shows only the residuals for SqFtTotLiving, but there are drastic improvements in the F-scoe, Langrange multiplier, and their associated p-values. For example, the p-value for the score increased from 2e-96 to 0.009. That leaves the p-value less than one order of magnitude away from meeting the requirement. Although it isn't success, it's a huge step.\n",
    "4. **Multi-collinearity: passed** The multi-collinearity assumption, as measured by VIF metrics is met. The average VIF is roughly 1.3. Given that anything below 5 is considered passable, this one is a pass. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model inputs and output\n",
    "inputs = ['SqFtTotLiving_log', 'Basement_Finished', 'Porch_Open', 'Porch_Closed', 'Porch_Both', 'Heating_ElecBB', 'Heating_FloorWall', 'Heating_HeatPump', 'Heating_HotWater','Heating_Radiant']\n",
    "output = 'SalePrice_log'\n",
    "\n",
    "# Run user-defined functions to create model and subsequently check the assumptions of linear regression\n",
    "model, df_model = produce_model(df_main, inputs, output)\n",
    "results = check_assumptions(model, df_model, output, verbose=True, feature_to_plot='SqFtTotLiving_log')\n",
    "performance_metrics = performance_metrics.append(results)\n",
    "\n",
    "print(model.summary())\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantify The Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantify the import of coefficients given log-scaled output: create DataFrame with results\n",
    "##### Create column to indicate whether a feature was log-transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(model.params).reset_index()\n",
    "results.columns = ['attribute', 'coeff']\n",
    "results['log_transformed?'] = ['_log' in x for x in results.attribute]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For feature that **were not** log transformed: calculate the \"Percent change in SalePrice per *UNIT* input increase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse log calculation\n",
    "exp_func = lambda x: np.round(10**x, 2)-1\n",
    "results['% change in SalePrice per *unit* input increase'] = results['coeff'].apply(exp_func)\n",
    "\n",
    "# Set intercept equal to NA as it cannot be interpretted this way\n",
    "results.iloc[0, -1] = 'NA'\n",
    "\n",
    "# Set features that *were* log transformed equal to NA. They need their own column\n",
    "results.loc[results['log_transformed?']==True, '% change in SalePrice per *unit* input increase'] = 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For feature that **were** log transformed: calculate the \"Percent change in SalePrice per *PERCENT* input increase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store converted values\n",
    "logged_inputs_converted = []\n",
    "\n",
    "# Loop over all features\n",
    "for row in range(results.shape[0]):\n",
    "    \n",
    "    # Enter statement if a feature was log transformed\n",
    "    if results.iloc[row]['log_transformed?'] == True:\n",
    "        \n",
    "        # 1.01 corresponds to a 1% increase\n",
    "        x = 1.01**results.iloc[row]['coeff']\n",
    "        logged_inputs_converted.append(x-1)\n",
    "    \n",
    "    else:\n",
    "        # This is not a valid metric for inputs that weren't transformed. Set to NA\n",
    "        logged_inputs_converted.append('NA')\n",
    "results['% change in SalePrice per *percent* input increase']  = logged_inputs_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "#### Enclose the porch\n",
    "Take note of the **Porch_** metrics in the *Results* table. Note that these are one-hot encoded, with the default being no porch at all. To put it into context, homes with *Open* porches tend to sell for roughly 8% more than homes without a porch at all. However, that number jumps to 27% for homes with *Enclosed* porches, meaning that homes with *enclosed* porches sell for nearly 20% more than homes with *open* porches. It looks like it's time to enclose that porch of yours!\n",
    "\n",
    "#### Finish the basement\n",
    "Refer to the **Basement_Finished** row in the *Results* table. This is one-hot encoded to indicate whether or not a basement is finished or unfinished. Note that this excludes homes with no basement at all. As we can see, homes with finished basements typically sell for 18% more than homes with unfinished basements. Maybe it's worth the time and money to get it finished up. \n",
    "\n",
    "#### Choosing a heating system\n",
    "Refer to the **Heating_** rows in the *Results* table. These are one-hot encoded values, with the default being Forced Air, given that it is by far the most common. It is used in over 75% of the 18,000 homes analyzed. Here, the results are unrealistically extreme. Instead of looking at the degree to which a heating system can impact home price, let's instead view it as a *trend*. The heating sources in the most expensive sales are Radiant and Hot Water. Heat pump and Floor-Wall are also correlated with a higher price than Forced Air. Coming in last, Electric Baseboard heating is associated with the lowest sale prices of homes. \n",
    "\n",
    "### Consider an add-on\n",
    "Amongst the strongest predictors of sale price is the **Livable Square Feet** in a home. This is not surprising. The relationship is striking: for every 10% increase in square footage, there is a 7% increase in home value. That's substantial. It is also a possible explanation as to why finishing the basement can have such a positive impact. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
